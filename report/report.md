# 美国总统Twitter辩论

## 目录

[TOC]

## 1. 引言

2020年的美国总统大选引来了同学们的不少关注，一方面是不同总统对华人的态度有所不同，可能会影响同学们求学的难度，另一方面是两位总统在Twitter上的风格截然不同，Donald Trump从2016年当选总统以来，一直被戏称为“Twitter治国”，他的tweet带有鲜明的个人情感因素以及特征，与总统一词的印象大相径庭，最标志性的特征就是他的全大写单词。Trump的竞争对手Joe Biden被Trump称为'Sleepy Joe'，是因为Biden表现的非常健忘，其tweet虽然没有Trump那么明显的风格特征，但是依然会语出惊人。于是，就想到了如果总统辩论采用的是Twitter的形式的话，会是怎样的场景。

随着自然语言处理的进步，现在的模型不仅仅能够做到一些基本的对话功能，也能够通过不同的语料库针对不同的特点进行建模，使得对话能够拥有自己的灵魂与特征。同时，Twitter作为时下流行的社交软件，其用户发送的文本数量大，且具有自己的特色，很多流行语以及有趣的梗都是从Twitter的用户发送的文本中产生的。Twitter限定其用户发送的Tweet字数小于120字，因此也是短文本生成与判别的很好的语料库。

要做到让这两位个性鲜明的总统能够用他们的Twitter风格进行对话，实际上需要对两者的语料库分别建立模型，才能生成出有自己风格特色的tweet。在这个项目中，我们采用了BERT-Tiny模型，以两位总统的2020年1月至6月发过的tweet为语料库，采用RoBERTa的训练方式对原始模型继续预训练。训练完成后，用生成式的方法(seq2seq模型)产生tweet。

## 2. 相关工作

### 2.1 对话系统

对话系统大致可分为两种:

1. 任务导向型(task-oriented)对话系统
2. 非任务导向型(non-task-oriented)对话系统(也称为聊天机器人)

任务导向型的系统旨在帮助用户完成实际具体的任务，例如帮助用户找寻商品，预订酒店餐厅等。

非任务导向的对话系统与人类交互，提供合理的回复和娱乐消遣功能，通常情况下主要集中在开放的领域与人交谈。而在这个任务中，我们需要用到的是非任务导向的对话系统，其方法主要可以分为两种：

1. 生成方法，例如序列到序列模型（seq2seq），在对话过程中产生合适的回复，生成型聊天机器人目前是研究界的一个热点，和检索型聊天机器人不同的是，它可以生成一种全新的回复，因此相对更为灵活，但它也有自身的缺点，比如有时候会出现语法错误，或者生成一些没有意义的回复；
2. 基于检索的方法，从事先定义好的索引中进行搜索，学习从当前对话中选择回复。检索型方法的缺点在于它过于依赖数据质量，如果选用的数据质量欠佳，那就很有可能前功尽弃。

因为BERT本身是一个seq2seq的模型，采用生成方法是更加自然的选择。

### 2.2 BERT

BERT的全称是Bidirectional Encoder Representation from Transformers，即双向Transformer的Encoder。模型的主要创新点是self-attention机制以及pre-train的方法，即用了Masked LM和Next Sentence Prediction两种方法分别捕捉词语和句子级别的表示。

#### 2.2.1 模型结构

Transformer的结构本质是由一个encoder和一个decoder组成的，其中的注意力机制是Transformer好用的原因。

encoder层由一层多头注意力和一层全连接的前向传播层组成，该多头注意力层就是BERT创新的self-attention机制。

decode层由一层masked多头注意力层，一层多头注意力层和一层全连接的前向传播层组成。这两层注意力层分别是self-attention和encoder-decoder注意力。

![](D:\Study\College\Junior\NLP\Project\report\pic\transformer.png)

在论文中，BERT所使用的注意力机制被叫做“归一化的点积注意力”。在实现过程中，该注意力机制可以理解为将问询以及一组键值对映射起来，并最终输出注意力值，这些问询以及键值对都是由向量构成的。通常来说，习惯将问询组成一个矩阵$Q$，将键值对分别组成矩阵$K$和$V$，注意力的计算公式如下：
$$
Attention(Q, K, V) = softmax(\frac{QK^T}{\sqrt{d_k}})V
$$

#### 2.2.2 预训练方法

BERT采用了两种预训练的方法：Masked Language Model以及Next Sentence Prediction。

MLM是指将随机的一个单词用[MASK]标识符代替，模型的任务就是预测这个[MASK]原来是什么单词。BERT对这个训练方法做的改进是当这个单词被选为mask单词时，80%的概率将这个单词替换为[MASK]，10%的概率将这个单词随机替换成其他单词，10%的概率不对这个单词做任何改动。这样的mask时候的trick能够帮助减少[MASK]这个标识符对模型的影响，同时，也能减少因mask使得单词没有出现过导致词嵌入没有被训练的概率。

NSP是指输入两个句子

## 3. 模型

